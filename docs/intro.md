---
sidebar_position: 1
---

# Humanoid Robotics: From Fundamentals to Autonomy

Welcome to this comprehensive guide on building autonomous humanoid robots. This documentation covers the complete stack from ROS 2 middleware fundamentals through simulation, perception, and vision-language-action systems.

## Learning Path

This documentation is organized into four progressive modules:

### Module 1: ROS 2 Fundamentals
Learn the middleware that coordinates humanoid robot software—nodes, topics, services, and URDF robot descriptions.

**Key Topics**: ROS 2 graph architecture, Python (rclpy) development, humanoid URDF modeling

### Module 2: The Digital Twin
Explore physics simulation with Gazebo, photorealistic rendering with Unity, and sensor modeling for synthetic data generation.

**Key Topics**: Physics engines, rendering pipelines, LiDAR/camera/IMU simulation, SDF worlds

### Module 3: The AI-Robot Brain
Understand NVIDIA Isaac tools for perception and navigation—photorealistic simulation, GPU-accelerated VSLAM, and Nav2 path planning.

**Key Topics**: Isaac Sim, cuVSLAM, Nav2 architecture, bipedal navigation challenges

### Module 4: Vision-Language-Action
Learn how VLA systems integrate speech recognition, LLM planning, and ROS 2 action execution for natural language robot control.

**Key Topics**: VLA architecture, Whisper STT, LLM task decomposition, behavior trees

## Prerequisites

- Basic Python programming knowledge
- Familiarity with Linux command line
- Understanding of fundamental robotics concepts

## Getting Started

Begin with [Module 1: ROS 2 Fundamentals](/docs/module-1-ros2-fundamentals/intro) to build foundational knowledge, then progress through subsequent modules.

## Code Examples

Runnable code examples are available in `/static/code-examples/` organized by module.
