---
sidebar_position: 0
title: "Introduction"
description: "Overview of Vision-Language-Action systems for humanoid robots"
---

# Vision-Language-Action (VLA) Systems

This module explores how modern AI enables humanoid robots to understand natural language commands and translate them into physical actions—the convergence of computer vision, large language models, and robotic control.

## What You'll Learn

- **VLA Foundations**: The three-stage architecture integrating vision, language, and action
- **Voice-to-Action**: How speech recognition (Whisper) enables natural command interfaces
- **LLM Planning**: Using large language models for task decomposition and planning
- **ROS 2 Actions**: Translating plans into executable robot behaviors
- **Capstone**: End-to-end autonomous humanoid scenario from voice to action

## Prerequisites

This module builds on concepts from:
- [Module 1: ROS 2 Fundamentals](/docs/module-1-ros2-fundamentals/intro) — Actions, services, and message types
- [Module 3: The AI-Robot Brain](/docs/module-3-ai-brain/intro) — Navigation and perception fundamentals

## The VLA Revolution

Traditional robot programming required explicit coding for every scenario. A robot that could pick up a cup needed specific code for cups—and different code for bottles, books, or tools.

VLA systems change this fundamentally. By combining:
- **Vision encoders** that understand what the robot sees
- **Language models** that understand what humans want
- **Action decoders** that know how robots move

...we create systems that generalize. Tell a VLA robot to "pick up the red cup" or "hand me that book"—and it understands, plans, and acts, without scenario-specific programming.

This module teaches the architecture and integration patterns that make this possible.
